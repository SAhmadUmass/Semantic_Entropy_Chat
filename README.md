# Semantic Entropy Chat

![Python](https://img.shields.io/badge/python-3.7%2B-blue.svg)
![OpenAI](https://img.shields.io/badge/OpenAI-API-blue.svg)

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [How It Works](#how-it-works)
- [Example](#example)
- [Understanding the Results](#understanding-the-results)
- [Limitations](#limitations)
- [Credits](#credits)

## Introduction

**Semantic Entropy Chat** is a Python tool designed to evaluate the reliability of responses generated by Large Language Models (LLMs) like GPT-4. By calculating semantic entropy over multiple generated answers, it helps detect potential hallucinationsâ€”instances where the model provides plausible but incorrect information.

## Features

- **Multiple Answer Generation**: Generates diverse responses using high-temperature sampling to capture the model's uncertainty.
- **Semantic Clustering**: Groups responses based on meaning using embeddings and agglomerative clustering.
- **Semantic Entropy Calculation**: Measures uncertainty over the distribution of meaning clusters.
- **Hallucination Prediction**: Flags responses as likely hallucinations based on entropy thresholds.

## How It Works

1. **Generate Multiple Answers**: For a given question, the model generates several responses with high variability.
2. **Retrieve Token Log Probabilities**: Each response includes token-level log probabilities to calculate sequence probabilities.
3. **Semantic Clustering**: Responses are embedded and clustered based on semantic similarity.
4. **Compute Cluster Probabilities**: Sum the sequence probabilities within each cluster.
5. **Calculate Semantic Entropy**: Quantify uncertainty using the entropy of cluster probabilities.
6. **Predict Hallucination**: Determine the likelihood of hallucination based on the entropy value.

### Prerequisites

- **Python 3.7+**
- **OpenAI API Key**: Obtain from [OpenAI](https://platform.openai.com/account/api-keys)

## Example

### Input
~~~
Question: Who wrote the play Hamlet?
~~~
### Output
~~~
Question: Who wrote the play Hamlet?

Generated Answers:
Answer 1: William Shakespeare
Answer 2: William Shakespeare
Answer 3: William Shakespeare wrote Hamlet.
Answer 4: William Shakespeare is the author of Hamlet.
Answer 5: Hamlet was written by William Shakespeare.

Clusters:
Cluster 0: ['William Shakespeare', 'William Shakespeare', 'William Shakespeare wrote Hamlet.', 'William Shakespeare is the author of Hamlet.', 'Hamlet was written by William Shakespeare.']

Cluster Probabilities:
Cluster 0: 1.0

Semantic Entropy: 0.0

The model's answer is likely reliable.
~~~

## Understanding the Results

- **Generated Answers**: Multiple responses generated by the LLM for the given question.
- **Clusters**: Groups of semantically similar answers based on their embeddings.
- **Cluster Probabilities**: The likelihood of each cluster calculated from the sequence probabilities of the responses within them.
- **Semantic Entropy**: A measure of uncertainty derived from the distribution of cluster probabilities. Lower entropy indicates higher confidence in the responses, while higher entropy suggests uncertainty.
- **Hallucination Prediction**: Based on the semantic entropy and a predefined threshold, the system predicts whether the model's answer is likely reliable or a hallucination.

## Limitations

- **Numerical Data Handling**: Embeddings may not effectively differentiate distinct numerical identifiers (e.g., PMIDs), potentially leading to inaccurate clustering.
- **Clustering Sensitivity**: The quality of semantic clustering depends on the choice of embedding model and clustering threshold, which may require fine-tuning.
- **Work in Progress**: **Token Probability Analysis** is currently being refined to enhance the accuracy of semantic entropy calculations. Future updates will incorporate more sophisticated probability assessments to better detect hallucinations.

## Credits

This project is based on the research conducted by [@UniofOxford](https://github.com/UniofOxford) and [@oatml_oxford](https://github.com/oatml_oxford).

- **Paper**: [Semantic Entropy](https://bit.ly/semantic-entropy)
- **Blog Post**: [Semantic Entropy Blog](https://bit.ly/semantic-entropy-blog)
- **Code**: [Semantic Entropy Code](https://bit.ly/semantic-entropy-code)

